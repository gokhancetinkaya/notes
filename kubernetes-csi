Install CSI driver
helm repo add hpe https://hpe-storage.github.io/co-deployments/
helm repo update
kubectl create ns vendor


For MicroK8s:
helm install -n vendor csi hpe/hpe-csi-driver --set kubeletRootDir=/var/snap/microk8s/common/var/lib/kubelet


?? Charmed Kubernetes:
helm install -n vendor csi hpe/hpe-csi-driver


kubectl get csidrivers
kubectl get csinodes

vim secret.yaml


apiVersion: v1
kind: Secret
metadata:
  name: csi
  namespace: vendor
stringData:
  serviceName: nimble-csp-svc
  servicePort: "8080"
  backend: 10.10.10.40
  username: admin
  password: adminadmin


kubectl create -f secret.yaml

Create a StorageClass and PVC
vim my-storageclass.yaml


apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: my-storageclass
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/controller-expand-secret-name: csi
  csi.storage.k8s.io/controller-expand-secret-namespace: vendor
  csi.storage.k8s.io/controller-publish-secret-name: csi
  csi.storage.k8s.io/controller-publish-secret-namespace: vendor
  csi.storage.k8s.io/node-publish-secret-name: csi
  csi.storage.k8s.io/node-publish-secret-namespace: vendor
  csi.storage.k8s.io/node-stage-secret-name: csi
  csi.storage.k8s.io/node-stage-secret-namespace: vendor
  csi.storage.k8s.io/provisioner-secret-name: csi
  csi.storage.k8s.io/provisioner-secret-namespace: vendor
  accessProtocol: fc
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate


kubectl apply -f my-storageclass.yaml
kubectl get sc

vim my-pvc.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi


kubectl apply -f my-pvc.yaml
kubectl get pvc
kubectl get pv

Attach a workload
vim nginx.yaml


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
    - name: nginx-storage
      persistentVolumeClaim:
        claimName: my-pvc
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-storage


kubectl apply -f nginx.yaml
kubectl exec -it nginx -- bash
echo “test” > /usr/share/nginx/html/index.html
exit

Expand volume
vim my-pvc-expand.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi


kubectl apply -f my-pvc-expand.yaml
watch kubectl get -f my-pvc.yaml


kubectl exec -it nginx -- bash
df -h
cat /usr/share/nginx/html/index.html
exit

Deploy CSI snapshotter, create a VolumeSnapshotClass and a VolumeSnapshot
git clone https://github.com/kubernetes-csi/external-snapshotter
kubectl create -f external-snapshotter/client/config/crd
kubectl create -f external-snapshotter/deploy/kubernetes/snapshot-controller

vim my-volumesnapshotclass.yaml


apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: my-volumesnapshotclass
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: "true"
driver: csi.hpe.com
deletionPolicy: Delete
parameters:
  csi.storage.k8s.io/snapshotter-secret-name: csi
  csi.storage.k8s.io/snapshotter-secret-namespace: vendor


kubectl create -f my-volumesnapshotclass.yaml

vim my-volumesnapshot.yaml


apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: my-snapshot
spec:
  source:
    persistentVolumeClaimName: my-pvc


kubectl create -f my-volumesnapshot.yaml
kubectl get -f my-volumesnapshot.yaml

Create a new PVC from a VolumeSnapshot
vim my-pvc-from-snapshot.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-from-snapshot
spec:
  dataSource:
    name: my-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi


kubectl create -f my-pvc-from-snapshot.yaml
kubectl get -f my-pvc-from-snapshot.yaml

Create a new PVC from an existing PVC
vim my-pvc-from-pvc.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-from-pvc
spec:
  dataSource:
    name: my-pvc
    kind: PersistentVolumeClaim
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi


kubectl create -f my-pvc-from-pvc.yaml

Create a raw block device and attach a workload
vim my-block-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-block-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  volumeMode: Block


kubectl create -f my-block-pvc.yaml

vim nginx-block.yaml


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
    - name: nginx-storage
      persistentVolumeClaim:
        claimName: my-block-pvc
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeDevices:
      - name: volume
        devicePath: /dev/xvda


kubectl create -f nginx-block.yaml
kubectl get pod/nginx-block.yaml -w
kubectl logs -f pods/nginx-block





-

# Deploy Rook

kubectl create ns rook-ceph

helm repo add rook-release https://charts.rook.io/release

helm repo update

helm install --namespace rook-ceph rook-ceph rook-release/rook-ceph

kubectl get pods -n rook-ceph -w

vim ceph-cluster.yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/ceph:v15.2.4
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 10Gi
  storage:
   storageClassDeviceSets:
    - name: set1
      count: 3
      portable: false
      tuneDeviceClass: false
      encrypted: false
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: 10Gi
          volumeMode: Block
          accessModes:
            - ReadWriteOnce

kubectl create -f ceph-cluster.yaml

kubectl get pods -n rook-ceph -w

kubectl get pvc -n rook-ceph -o wide

-

# Using ephemeral local volumes

vim my-inline.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"               
        name: my-mount
  volumes:
  - name: my-mount
    csi:
      driver: csi.hpe.com
      volumeAttributes:
        csi.storage.k8s.io/ephemeral: "true"
        inline-volume-secret-name: csi
        inline-volume-secret-namespace: vendor
        size: 10Gi

kubectl create -f my-inline.yaml
kubectl get -f my-inline.yaml -w

kubectl exec -it pod/my-pod --bash
cd /usr/share/nginx/html
echo "test" > index.html
exit

kubectl replace --force -f my-inline.yaml
kubectl get -f my-inline.yaml -w

kubectl exec -it pod/my-pod -- bash
cat /usr/share/nginx/html/index.html
exit

-

# Using generic ephemeral volumes

vim my-ephemeral.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"               
        name: my-mount
  volumes:
  - name: my-mount
    ephemeral:
      volumeClaimTemplate:
        metadata:
          labels:
            app: myfrontend
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 10Gi

kubectl create -f my-ephemeral.yaml
kubectl get -f my-ephemeral.yaml -w

kubectl get pvc -l app=myfrontend

kubectl exec -it pod/my-pod -- bash
cd /usr/share/nginx/html
echo "test" > index.html
exit

kubectl replace --force -f my-ephemeral.yaml
kubectl get -f my-ephemeral.yaml -w

kubectl exec -it pod/my-pod -- bash
cat /usr/share/nginx/html/index.html
exit

kubectl get pvc -l app=myfrontend

---

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
    - name: nginx-storage
      persistentVolumeClaim:
        claimName: my-pvc
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-storage
