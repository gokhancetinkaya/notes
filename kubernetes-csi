Storage appliances tested:
HPE Alletra 6030 (Software version: 6.1.1.300-1028597-opt)
HPE Alletra 9060 (Software version: 9.5.11)

OS: Ubuntu Server 22.04.2
Kubernetes version: 1.27.4
hpe-csi-driver Helm chart version: 2.3.0

OS download link: https://releases.ubuntu.com/22.04.2/ubuntu-22.04.2-live-server-amd64.iso

Update system
sudo apt update && sudo apt upgrade -y

Install LXD
sudo snap refresh snapd
sudo snap install lxd --channel latest/stable
sudo lxd init

Install MAAS
sudo snap install maas --channel=3.3/stable
sudo snap install maas-test-db
sudo maas init region+rack --database-uri maas-test-db:/// --maas-url http://10.10.20.201:5240/MAAS
sudo maas createadmin --username admin --password admin --email admin@localhost
ssh-keygen
sudo maas apikey --username admin > ~/admin-api-key

Enable DHCP on 192.168.1.0/24 network

Configure MAAS as LXD host

Enable routing for LXD VMs through MAAS
sudo vim /etc/sysctl.conf
net.ipv4.ip_forward=1
sudo iptables -t nat -A POSTROUTING -o eno5np0 -j MASQUERADE
sudo apt install iptables-persistent
sudo iptables-save > /etc/iptables/rules.v4

Create Juju VM

Install Juju
sudo snap install juju --channel=2.9/stable --classic
juju add-cloud
Select cloud type: maas
Enter a name for your maas cloud: maas-cloud
Enter the API endpoint url: http://10.10.20.201:5240/MAAS


juju add-credential maas-cloud
Enter credential name: maas-cloud-creds
Enter maas-oauth: <maas admin api key>
juju bootstrap maas-cloud --constraints tags=juju

Install Kubernetes
juju add-model -c maas-cloud-default kubernetes
juju switch maas-cloud-default:kubernetes
juju download charmed-kubernetes --channel latest/stable
unzip charmed-kubernetes.bundle -d kubernetes
rm charmed-kubernetes.bundle
cd kubernetes

Deploy bundle
juju deploy ./bundle.yaml

Install CSI driver
Charmed Kubernetes, on a node:
sudo snap install helm --classic


For MicroK8s:
#helm is already installed


helm repo add hpe https://hpe-storage.github.io/co-deployments/
helm repo update
kubectl create ns vendor


For MicroK8s:
helm install -n vendor csi hpe/hpe-csi-driver --set kubeletRootDir=/var/snap/microk8s/common/var/lib/kubelet


Charmed Kubernetes, on a node:
helm install -n vendor csi hpe/hpe-csi-driver


kubectl get csidrivers
kubectl get csinodes

Alletra 6000
vim secret.yaml


apiVersion: v1
kind: Secret
metadata:
  name: csi
  namespace: vendor
stringData:
  serviceName: nimble-csp-svc
  servicePort: "8080"
  backend: 10.10.10.40
  username: admin
  password: adminadmin

Alletra 9000
vim secret.yaml


apiVersion: v1
kind: Secret
metadata:
  name: csi
  namespace: vendor
stringData:
  serviceName: primera3par-csp-svc
  servicePort: "8080"
  backend: 10.10.10.60
  username: 3paradm
  password: 3pardata

kubectl create -f secret.yaml
kubectl get secret -n vendor

Create StorageClass

Alletra 6000
vim my-storageclass.yaml


apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: my-storageclass
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/controller-expand-secret-name: csi
  csi.storage.k8s.io/controller-expand-secret-namespace: vendor
  csi.storage.k8s.io/controller-publish-secret-name: csi
  csi.storage.k8s.io/controller-publish-secret-namespace: vendor
  csi.storage.k8s.io/node-publish-secret-name: csi
  csi.storage.k8s.io/node-publish-secret-namespace: vendor
  csi.storage.k8s.io/node-stage-secret-name: csi
  csi.storage.k8s.io/node-stage-secret-namespace: vendor
  csi.storage.k8s.io/provisioner-secret-name: csi
  csi.storage.k8s.io/provisioner-secret-namespace: vendor
  accessProtocol: fc
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate

Get CPG name from Alletra 9000 for the Cinder configuration below (hpe3par_cpg):
ssh 3paradm@10.10.10.60
showcpg

Alletra 9000
vim my-storageclass.yaml


apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: my-storageclass
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: csi.hpe.com
parameters:
  csi.storage.k8s.io/fstype: xfs
  csi.storage.k8s.io/controller-expand-secret-name: csi
  csi.storage.k8s.io/controller-expand-secret-namespace: vendor
  csi.storage.k8s.io/controller-publish-secret-name: csi
  csi.storage.k8s.io/controller-publish-secret-namespace: vendor
  csi.storage.k8s.io/node-publish-secret-name: csi
  csi.storage.k8s.io/node-publish-secret-namespace: vendor
  csi.storage.k8s.io/node-stage-secret-name: csi
  csi.storage.k8s.io/node-stage-secret-namespace: vendor
  csi.storage.k8s.io/provisioner-secret-name: csi
  csi.storage.k8s.io/provisioner-secret-namespace: vendor
  accessProtocol: fc
  cpg: SSD_r6
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate

kubectl create -f my-storageclass.yaml
kubectl get sc

Create PVC
vim my-pvc.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi


kubectl create -f my-pvc.yaml
kubectl get pvc
kubectl get pv

Attach a workload
vim nginx.yaml


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
    - name: nginx-storage
      persistentVolumeClaim:
        claimName: my-pvc
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-storage


kubectl create -f nginx.yaml
kubectl get pods -w
kubectl exec -it nginx -- bash
df -h
echo “test” > /usr/share/nginx/html/index.html
exit

Expand volume
vim my-pvc-expand.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi


kubectl create -f my-pvc-expand.yaml
kubectl get -f my-pvc-expand.yaml -w


kubectl exec -it nginx -- bash
df -h
cat /usr/share/nginx/html/index.html
exit

Deploy CSI snapshotter
git clone https://github.com/kubernetes-csi/external-snapshotter


rm external-snapshotter/client/config/crd/kustomization.yaml
rm external-snapshotter/deploy/kubernetes/snapshot-controller/kustomization.yaml


kubectl create -f external-snapshotter/client/config/crd
kubectl create -f external-snapshotter/deploy/kubernetes/snapshot-controller

Create VolumeSnapshotClass
vim my-volumesnapshotclass.yaml


apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: my-volumesnapshotclass
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: "true"
driver: csi.hpe.com
deletionPolicy: Delete
parameters:
  csi.storage.k8s.io/snapshotter-secret-name: csi
  csi.storage.k8s.io/snapshotter-secret-namespace: vendor


kubectl create -f my-volumesnapshotclass.yaml
kubectl get -f my-volumesnapshotclass.yaml

Create VolumeSnapshot
vim my-volumesnapshot.yaml


apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: my-volumesnapshot
spec:
  source:
    persistentVolumeClaimName: my-pvc


kubectl create -f my-volumesnapshot.yaml
kubectl get -f my-volumesnapshot.yaml

Create a new PVC from the VolumeSnapshot
vim my-pvc-from-snapshot.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-from-snapshot
spec:
  dataSource:
    name: my-volumesnapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi


kubectl create -f my-pvc-from-snapshot.yaml
kubectl get -f my-pvc-from-snapshot.yaml

Create a new PVC from an existing PVC
vim my-pvc-from-pvc.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc-from-pvc
spec:
  dataSource:
    name: my-pvc
    kind: PersistentVolumeClaim
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi


kubectl create -f my-pvc-from-pvc.yaml
kubectl get -f my-pvc-from-pvc.yaml

Create a raw block device
vim my-block-pvc.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-block-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  volumeMode: Block


kubectl create -f my-block-pvc.yaml

Attach a workload to the raw block device
vim nginx2.yaml


apiVersion: v1
kind: Pod
metadata:
  name: nginx2
spec:
  volumes:
    - name: nginx-storage
      persistentVolumeClaim:
        claimName: my-block-pvc
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeDevices:
      - name: nginx-storage
        devicePath: /dev/xvda


kubectl create -f nginx2.yaml

Using inline ephemeral volumes
vim my-ephemeral.yaml


apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"               
        name: my-mount
  volumes:
  - name: my-mount
    csi:
      driver: csi.hpe.com
      volumeAttributes:
        csi.storage.k8s.io/ephemeral: "true"
        inline-volume-secret-name: csi
        inline-volume-secret-namespace: vendor
        size: 10Gi


kubectl create -f my-ephemeral.yaml

Using generic ephemeral volumes
vim my-ephemeral2.yaml


apiVersion: v1
kind: Pod
metadata:
  name: my-pod2
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"               
        name: my-mount
  volumes:
  - name: my-mount
    ephemeral:
      volumeClaimTemplate:
        metadata:
          labels:
            app: myfrontend
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 10Gi


kubectl create -f my-ephemeral2.yaml
